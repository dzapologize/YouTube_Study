# SuperFastPython.com
# download all files from a website sequentially
from multiprocessing.pool import ThreadPool
from os import makedirs
from os.path import basename
from os.path import join
from urllib.request import Request,urlopen
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import as_completed
from bs4 import BeautifulSoup
import requests
import os

# url of html page that lists all files to download
url = 'https://downloadly.net/2023/02/91199/02/nft-investing-masterclass-pro-tips-about-nft-investing/14/?#/91199-udemy-182304021602.html'
# local directory to save all files on the html page

path = 'tmp'

from urllib.request import Request, urlopen

# load a file from a URL, returns content of downloaded file
def download_url(urlpath):
    # open a connection to the server
    req = Request(url=url,headers={'User-Agent': 'Mozilla/5.0'})
    webpage = urlopen(req).read()
    return webpage

# decode downloaded html and extract all <a href=""> links
def get_urls_from_html(content):
    # decode the provided content as ascii text
    html = content.decode('utf-8')
    # parse the document as best we can
    soup = BeautifulSoup(html, 'html.parser')
    # find all all of the <a href=""> tags in the document
    atags = soup.find_all('a')
    # get all href values (links) or None if not present (unlikely)
    return [t.get('href', None) for t in atags]

data = download_url(url)
makedirs(path, exist_ok=True)
links = get_urls_from_html(data)
the_link = []


def download_file(url):
    local_filename = url.split('/')[-1]
    path = local_filename.split('.')[0]
    os.makedirs(path,exist_ok=True)
    # NOTE the stream=True parameter below
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(path + '/' + local_filename, 'wb') as f:
            for chunk in r.iter_content(): 
                # If you have chunk encoded response uncomment if
                # and set chunk_size parameter to None.
                #if chunk: 
                f.write(chunk)
    return local_filename


for link in links:
    if link is not None and "_Downloadly.ir.rar" in link:
        download_file(link)



